{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "# P5: 从安然公司邮件中发现欺诈证据"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\elnin\\Anaconda2\\lib\\site-packages\\sklearn\\cross_validation.py:44: DeprecationWarning: This module was deprecated in version 0.18 in favor of the model_selection module into which all the refactored classes and functions are moved. Also note that the interface of the new CV iterators are different from that of this module. This module will be removed in 0.20.\n",
      "  \"This module will be removed in 0.20.\", DeprecationWarning)\n"
     ]
    }
   ],
   "source": [
    "#!/usr/bin/python\n",
    "\n",
    "import sys\n",
    "import pickle\n",
    "import matplotlib.pyplot\n",
    "import numpy as np\n",
    "sys.path.append(\"../tools/\")\n",
    "\n",
    "%matplotlib inline\n",
    "\n",
    "from feature_format import featureFormat, targetFeatureSplit\n",
    "from tester import dump_classifier_and_data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 任务0: 浏览数据级，看看数据集大致是什么样"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "count:  146  poi_count:  18 features count: 21\n",
      "all features coount: ['salary', 'to_messages', 'deferral_payments', 'total_payments', 'loan_advances', 'bonus', 'email_address', 'restricted_stock_deferred', 'total_stock_value', 'shared_receipt_with_poi', 'long_term_incentive', 'exercised_stock_options', 'from_messages', 'other', 'from_poi_to_this_person', 'from_this_person_to_poi', 'poi', 'deferred_income', 'expenses', 'restricted_stock', 'director_fees']\n"
     ]
    }
   ],
   "source": [
    "### Load the dictionary containing the dataset\n",
    "with open(\"final_project_dataset.pkl\", \"r\") as data_file:\n",
    "    data_dict = pickle.load(data_file)\n",
    "\n",
    "count, poi_count = 0,0\n",
    "all_features_list = set()\n",
    "for k,v in data_dict.items():\n",
    "    if v['poi'] == True: poi_count += 1\n",
    "    count += 1\n",
    "    for k1 in v.keys():\n",
    "        if not k1 in all_features_list:\n",
    "            all_features_list.add(k1)\n",
    "all_features_list = list(all_features_list)            \n",
    "print 'count: ',count,\" poi_count: \",poi_count, \"features count:\", len(all_features_list)\n",
    "print \"all features coount:\", all_features_list"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "数据集中共有人数 ** 146 ** 人，其中涉嫌欺诈的有 **18** 人, 特征个数是 **21** 个\n",
    "\n",
    "看看一个典型用户有哪些特征，其值是多少"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "salary  :  365788\n",
      "to_messages  :  807\n",
      "deferral_payments  :  NaN\n",
      "total_payments  :  1061827\n",
      "exercised_stock_options  :  NaN\n",
      "bonus  :  600000\n",
      "restricted_stock  :  585062\n",
      "shared_receipt_with_poi  :  702\n",
      "restricted_stock_deferred  :  NaN\n",
      "total_stock_value  :  585062\n",
      "expenses  :  94299\n",
      "loan_advances  :  NaN\n",
      "from_messages  :  29\n",
      "other  :  1740\n",
      "from_this_person_to_poi  :  1\n",
      "poi  :  False\n",
      "director_fees  :  NaN\n",
      "deferred_income  :  NaN\n",
      "long_term_incentive  :  NaN\n",
      "email_address  :  mark.metts@enron.com\n",
      "from_poi_to_this_person  :  38\n"
     ]
    }
   ],
   "source": [
    "for k,v in data_dict['METTS MARK'].items():\n",
    "    print k, \" : \", v"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 任务1: 选择你需要的特征\n",
    "\n",
    "选择所有的财务特征, 其中  email_address 是没有任何数据信息，所以可以考虑去掉"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "20\n"
     ]
    }
   ],
   "source": [
    "### Task 1: Select what features you'll use.\n",
    "### features_list is a list of strings, each of which is a feature name.\n",
    "### The first feature must be \"poi\".\n",
    "features_list = ['poi']\n",
    "# finance feature\n",
    "features_list += ['salary', 'deferral_payments', 'total_payments', 'loan_advances', 'bonus', \n",
    "                  'restricted_stock_deferred', 'deferred_income', 'total_stock_value', 'expenses', \n",
    "                  'exercised_stock_options', 'other', 'long_term_incentive', 'restricted_stock', \n",
    "                  'director_fees']\n",
    "# email feature\n",
    "features_list +=  ['to_messages', 'from_poi_to_this_person', 'from_messages', 'from_this_person_to_poi', 'shared_receipt_with_poi']\n",
    "\n",
    "print len(features_list)\n",
    "\n",
    "### Load the dictionary containing the dataset\n",
    "y, X = targetFeatureSplit(featureFormat(data_dict, features_list))\n",
    "X = np.array(X)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "试试用决策树大致筛选一下对数据的敏感性"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "clf.feature_importances_ :  [ 0.          0.          0.16475213  0.          0.17958795  0.          0.0543682\n",
      "  0.          0.06221847  0.05858439  0.08072355  0.10873641  0.12036185\n",
      "  0.          0.          0.08721566  0.          0.08345138  0.        ]\n",
      "importance  0  -  bonus  -  0.179587954314\n",
      "importance  1  -  total_payments  -  0.164752133256\n",
      "importance  2  -  restricted_stock  -  0.120361849464\n",
      "importance  3  -  long_term_incentive  -  0.108736407949\n",
      "importance  4  -  from_poi_to_this_person  -  0.0872156605424\n",
      "importance  5  -  from_this_person_to_poi  -  0.0834513822454\n",
      "importance  6  -  other  -  0.0807235517364\n",
      "importance  7  -  expenses  -  0.0622184676102\n",
      "importance  8  -  exercised_stock_options  -  0.0585843889079\n",
      "importance  9  -  deferred_income  -  0.0543682039745\n"
     ]
    }
   ],
   "source": [
    "from sklearn import tree\n",
    "clf = tree.DecisionTreeClassifier(random_state=12)\n",
    "clf = clf.fit(X, y)\n",
    "print \"clf.feature_importances_ : \", clf.feature_importances_\n",
    "idx_feature_importances = np.argsort(clf.feature_importances_)[::-1]\n",
    "for i in range(10):\n",
    "    idx = idx_feature_importances[i]\n",
    "    print \"importance \", i, \" - \", features_list[idx+1], \" - \", clf.feature_importances_[idx]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "由此可以看出，和poi关系大的，依次是\n",
    "\n",
    "- bonus: 奖金\n",
    "- total_payments: 总收入\n",
    "- restricted_stock: 受限股票\n",
    "- expenses: 花费\n",
    "- long_term_incentive: 长期激励\n",
    "- from_poi_to_this_person: 从poi邮箱发到此邮箱的邮件数量\n",
    "- exercised_stock_options: 行权数量\n",
    "- from_this_person_to_poi: 从此邮箱发到poi邮箱的邮件数量\n",
    "\n",
    "可以看出比较奇特的是，工资 salary 居然对结果影响不大"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['poi', 'bonus', 'total_payments', 'restricted_stock', 'long_term_incentive', 'from_poi_to_this_person', 'from_this_person_to_poi', 'other', 'expenses', 'exercised_stock_options', 'deferred_income']\n"
     ]
    }
   ],
   "source": [
    "new_features_list = ['poi']\n",
    "for i in range(10):\n",
    "    idx = idx_feature_importances[i]\n",
    "    new_features_list.append(features_list[idx+1])\n",
    "print new_features_list\n",
    "features_list = new_features_list\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 任务2: 删除异常值"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "由于上面发现的奇怪现象，这里首先检查salary的异常值。\n",
    "\n",
    "这里先检查 salary 和 bonus里面 有没有NaN的。如果看看，是否存在POI人。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "YEAGER F SCOTT  -  158403   NaN\n",
      "HIRKO JOSEPH  -  NaN   NaN\n",
      "\n",
      "nan_salary_count:  51  nan_bonus_count:  64\n"
     ]
    }
   ],
   "source": [
    "nan_salary_count = 0\n",
    "nan_bonus_count = 0\n",
    "poi_count_when_nan_salary_or_nan_bonu = 0\n",
    "for k,v in data_dict.items():\n",
    "    if v[\"salary\"] == 'NaN': nan_salary_count +=1\n",
    "    if v[\"bonus\"] == 'NaN': nan_bonus_count += 1\n",
    "    if v[\"salary\"] == 'NaN'or v[\"bonus\"] == 'NaN':\n",
    "        if v[\"poi\"] == True:\n",
    "            print k,\" - \", v[\"salary\"], \" \", v[\"bonus\"]\n",
    "print\n",
    "print \"nan_salary_count: \", nan_salary_count, \" nan_bonus_count: \", nan_bonus_count\n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "发现存在了不少 工资 或者 奖金 NaN, 但是这些里面存在POI人，不能说明是异常"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "max_bonus  97343619.0\n",
      "the max bonus person is  TOTAL\n"
     ]
    }
   ],
   "source": [
    "### Task 2: Remove outliers\n",
    "bonus_idx = 4\n",
    "\n",
    "max_bonus = np.amax(X[:,bonus_idx])\n",
    "print \"max_bonus \", max_bonus\n",
    "for k,v in data_dict.items():\n",
    "    if v[\"bonus\"] == max_bonus:\n",
    "        print \"the max bonus person is \", k"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "TOTAL 是总数，这个是典型的异常值，所以一定要拿掉这个值"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(144L, 10L)\n"
     ]
    }
   ],
   "source": [
    "data_dict.pop(\"TOTAL\", 0)\n",
    "y, X = targetFeatureSplit(featureFormat(data_dict, features_list))\n",
    "X = np.array(X)\n",
    "print X.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "看看 bonus或者salary 有没有小于0异常值"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "done\n"
     ]
    }
   ],
   "source": [
    "for k,v in data_dict.items():\n",
    "    if v[\"bonus\"] == 'NaN' or v[\"salary\"] == 'NaN':\n",
    "        continue\n",
    "    if v[\"bonus\"] < 0 or v[\"salary\"] < 0:\n",
    "        print \"the bonus or salary is less than 0 is %s -- %s,%s\"%(k,v[\"bonus\"],v[\"salary\"])\n",
    "print \"done\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "看来，没有小于0的，再看看有没有 bonus 和 salary 超级多的"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "the 2 max salary abd bonus is LAY KENNETH L -- 1072321,7000000\n",
      "the 2 max salary abd bonus is SKILLING JEFFREY K -- 1111258,5600000\n"
     ]
    }
   ],
   "source": [
    "for k,v in data_dict.items():\n",
    "    if v[\"salary\"] == 'NaN' or v[\"bonus\"] == 'NaN':\n",
    "        continue\n",
    "    if v[\"salary\"] > 1000000 and v[\"bonus\"] > 5000000:\n",
    "        print \"the 2 max salary abd bonus is %s -- %s,%s\"%(k,v[\"salary\"],v[\"bonus\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "发现有2个人，这分别是大名鼎鼎的 Kenneth Lay 和 Jeffrey Skilling，很显然，他们不是异常值，他们是正常值"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 任务3: 创建新特征"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "### Task 3: Create new feature(s)\n",
    "### Store to my_dataset for easy export below.\n",
    "my_dataset = data_dict\n",
    "old_features_list = features_list\n",
    "\n",
    "def isnan(num):\n",
    "    return num != num"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "增加 从poi邮箱中收取邮件的百分比 和 发送到poi邮箱中的百分比"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "for k,v in my_dataset.items():\n",
    "    if float(v['to_messages']) == 0.: \n",
    "        v['from_poi_to_this_ratio'] = 0.\n",
    "    else: \n",
    "        v['from_poi_to_this_ratio'] = float(v['from_poi_to_this_person']) / float(v['to_messages'])\n",
    "    if float(v['from_messages']) == 0.: \n",
    "        v['from_this_to_poi_ratio'] = 0.\n",
    "    else:\n",
    "        v['from_this_to_poi_ratio'] =  float(v['from_this_person_to_poi']) / float(v['from_messages'])\n",
    "    if isnan(v['from_poi_to_this_ratio']): v['from_poi_to_this_ratio'] = 'NaN'\n",
    "    if isnan(v['from_this_to_poi_ratio']): v['from_this_to_poi_ratio'] = 'NaN'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "增加 奖金和工资的倍数，也就是 `bonus / salary`。按照常识，这个一般奖金按照公司系数发放"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "for k,v in my_dataset.items():\n",
    "    #print '=========== ',k\n",
    "    if float(v['salary']) == 0.: \n",
    "        v['bonus_times'] = 0.\n",
    "    else: \n",
    "        v['bonus_times'] = float(v['bonus']) / float(v['salary'])\n",
    "    if isnan(v['bonus_times']): \n",
    "        v['bonus_times'] = 'NaN'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "features  (144L, 13L)\n",
      "lables  144\n"
     ]
    }
   ],
   "source": [
    "### Extract features and labels from dataset for local testing\n",
    "features_list = old_features_list + ['from_this_to_poi_ratio', 'from_poi_to_this_ratio', 'bonus_times']\n",
    "\n",
    "data = featureFormat(my_dataset, features_list, sort_keys = True)\n",
    "y, X = targetFeatureSplit(data)\n",
    "X = np.array(X)\n",
    "print \"features \", X.shape\n",
    "print \"lables \", len(y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "特征增加了，再次用决策树看看，重要相关特征有哪些"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "clf.feature_importances_ :  [ 0.04232804  0.          0.02817127  0.04232804  0.          0.          0.\n",
      "  0.27209809  0.27220666  0.08300395  0.25986395  0.          0.        ]\n",
      "importance  0  -  exercised_stock_options  -  0.272206660442\n",
      "importance  1  -  expenses  -  0.272098088247\n",
      "importance  2  -  from_this_to_poi_ratio  -  0.259863945578\n",
      "importance  3  -  deferred_income  -  0.0830039525692\n",
      "importance  4  -  long_term_incentive  -  0.042328042328\n",
      "importance  5  -  bonus  -  0.042328042328\n",
      "importance  6  -  restricted_stock  -  0.0281712685074\n",
      "importance  7  -  bonus_times  -  0.0\n",
      "importance  8  -  from_poi_to_this_ratio  -  0.0\n",
      "importance  9  -  other  -  0.0\n"
     ]
    }
   ],
   "source": [
    "clf = tree.DecisionTreeClassifier(random_state=12)\n",
    "clf = clf.fit(X, y)\n",
    "print \"clf.feature_importances_ : \", clf.feature_importances_\n",
    "idx_feature_importances = np.argsort(clf.feature_importances_)[::-1]\n",
    "for i in range(10):\n",
    "    idx = idx_feature_importances[i]\n",
    "    print \"importance \", i, \" - \", features_list[idx+1], \" - \", clf.feature_importances_[idx]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "根据上面的内容，我把特征的范围再缩小一下，只用决策树有用的特征"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['poi', 'exercised_stock_options', 'expenses', 'from_this_to_poi_ratio', 'deferred_income', 'long_term_incentive', 'bonus', 'restricted_stock']\n",
      "features  (141L, 7L)\n",
      "lables  141\n"
     ]
    }
   ],
   "source": [
    "new_features_list = ['poi']\n",
    "for i in range(7):\n",
    "    idx = idx_feature_importances[i]\n",
    "    new_features_list.append(features_list[idx+1])\n",
    "features_list = new_features_list\n",
    "print new_features_list\n",
    "\n",
    "data = featureFormat(my_dataset, features_list, sort_keys = True)\n",
    "y, X = targetFeatureSplit(data)\n",
    "X = np.array(X)\n",
    "print \"features \", X.shape\n",
    "print \"lables \", len(y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 任务4: 尝试各种各样的分类器模型\n",
    "\n",
    "首先拆分好数据，70%为训练集，30%用于验证。 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "done\n"
     ]
    }
   ],
   "source": [
    "### Task 4: Try a varity of classifiers\n",
    "### Please name your classifier clf for easy export below.\n",
    "### Note that if you want to do PCA or other multi-stage operations,\n",
    "### you'll need to use Pipelines. For more info:\n",
    "### http://scikit-learn.org/stable/modules/pipeline.html\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.metrics import accuracy_score, recall_score, precision_score, make_scorer\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.cross_validation import StratifiedShuffleSplit\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)\n",
    "pca = PCA(svd_solver='randomized', n_components=10)\n",
    "cv = StratifiedShuffleSplit(y_train, 100, random_state=42)\n",
    "print \"done\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 尝试朴素贝叶斯算法"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\elnin\\Anaconda2\\lib\\site-packages\\sklearn\\metrics\\classification.py:1115: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 due to no true samples.\n",
      "  'recall', 'true', average, warn_for)\n",
      "C:\\Users\\elnin\\Anaconda2\\lib\\site-packages\\sklearn\\metrics\\classification.py:1113: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 due to no predicted samples.\n",
      "  'precision', 'predicted', average, warn_for)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "before pca f1:  0.212257142857\n",
      "after pca f1:  0.138604761905\n"
     ]
    }
   ],
   "source": [
    "# GaussianNB\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "naive_clf = GaussianNB()\n",
    "parameters = {}\n",
    "naive_clf_grid = GridSearchCV(naive_clf, parameters, cv=cv, scoring='f1')\n",
    "naive_clf_grid.fit(X, y)\n",
    "print \"before pca f1: \", naive_clf_grid.best_score_\n",
    "\n",
    "pca_naive_clf = Pipeline([('pca', pca),('svc', naive_clf)])\n",
    "parameters = {'pca__n_components': range(1,5)}\n",
    "pca_naive_clf_grid = GridSearchCV(pca_naive_clf, parameters, cv=cv, scoring='f1')\n",
    "pca_naive_clf_grid.fit(X, y)\n",
    "print \"after pca f1: \", pca_naive_clf_grid.best_score_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 尝试决策树算法"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "before pca f1:  0.21149047619\n",
      "after pca f1:  0.244235714286\n"
     ]
    }
   ],
   "source": [
    "# DissionTree\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "tree_clf = DecisionTreeClassifier(random_state=12)\n",
    "parameters = {'max_depth': range(3,10)}\n",
    "tree_clf_grid = GridSearchCV(tree_clf, parameters, cv=cv, scoring='f1')\n",
    "tree_clf_grid.fit(X, y)\n",
    "print \"before pca f1: \", tree_clf_grid.best_score_\n",
    "\n",
    "pca_tree_clf = Pipeline([('pca', pca), ('svc', tree_clf)])\n",
    "parameters = {'pca__n_components': range(1,5),\n",
    "              'svc__max_depth': range(3,10)}\n",
    "pca_tree_clf_grid = GridSearchCV(pca_tree_clf, parameters, cv=cv, scoring='f1')\n",
    "pca_tree_clf_grid.fit(X, y)\n",
    "print \"after pca f1: \", pca_tree_clf_grid.best_score_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 尝试SVM算法\n",
    "\n",
    "很奇怪，我按照一下的方法，进入出不来结果，永远出不来结果，调整一下参数，偶尔能出来结果"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# svm\n",
    "from sklearn.svm import SVC\n",
    "svm_clf = SVC(random_state=42)\n",
    "parameters = {'kernel': ('rbf','linear'), \n",
    "              'C': [1,10]}\n",
    "svm_clf_grid = GridSearchCV(svm_clf, parameters, cv=cv, scoring='f1')\n",
    "svm_clf_grid.fit(X, y)\n",
    "print \"before pca f1: \", svm_clf_grid.best_score_\n",
    "\n",
    "pca_svm_clf = Pipeline([('pca', pca), ('svc', svm_clf)])\n",
    "parameters = {'pca__n_components': [3,5,7,9],\n",
    "              'svc__kernel': ['rbf','linear'], \n",
    "              'svc__C': [1,10]\n",
    "             }\n",
    "pca_svm_clf_grid = GridSearchCV(pca_svm_clf, parameters, cv=cv, scoring='f1')\n",
    "pca_svm_clf_grid.fit(X, y)\n",
    "print \"after pca f1: \", pca_svm_clf_grid.best_score_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 尝试一些集成学习算法，AdaBoost"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "before pca f1:  0.178\n",
      "after pca f1:  0.186\n"
     ]
    }
   ],
   "source": [
    "# AdaBoost\n",
    "from sklearn.ensemble import AdaBoostClassifier\n",
    "adaboost_clf = AdaBoostClassifier(random_state=42)\n",
    "parameters = {'n_estimators': [25,50,75,100],\n",
    "              'learning_rate': [0.01, 0.1, 1]}\n",
    "adaboost_clf_grid = GridSearchCV(adaboost_clf, parameters, cv=cv, scoring='f1')\n",
    "adaboost_clf_grid.fit(X, y)\n",
    "print \"before pca f1: \", adaboost_clf_grid.best_score_\n",
    "\n",
    "pca_adaboost_clf = Pipeline([('pca', pca), ('svc', adaboost_clf)])\n",
    "parameters = {'pca__n_components': range(1,5),\n",
    "              'svc__n_estimators': [25,50,75,100],\n",
    "              'svc__learning_rate': [0.01, 0.1, 1]}\n",
    "pca_adaboost_clf_grid = GridSearchCV(pca_adaboost_clf, parameters, cv=cv, scoring='f1')\n",
    "pca_adaboost_clf_grid.fit(X, y)\n",
    "print \"after pca f1: \", pca_adaboost_clf_grid.best_score_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 尝试一些集成学习算法，RandomForest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "before pca f1:  0.207\n",
      "after pca f1:  0.241333333333\n"
     ]
    }
   ],
   "source": [
    "# RandomForest\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "forest_clf = AdaBoostClassifier(random_state=42)\n",
    "parameters = {'n_estimators': [5,10,15,20]}\n",
    "forest_clf_grid = GridSearchCV(forest_clf, parameters, cv=cv, scoring='f1')\n",
    "forest_clf_grid.fit(X, y)\n",
    "print \"before pca f1: \", forest_clf_grid.best_score_\n",
    "\n",
    "pca_forest_clf = Pipeline([('pca', pca), ('svc', forest_clf)])\n",
    "parameters = {'pca__n_components': range(1,5),\n",
    "              'svc__n_estimators': [5,10,15,20]}\n",
    "pca_forest_clf_grid = GridSearchCV(pca_forest_clf, parameters, cv=cv, scoring='f1')\n",
    "pca_forest_clf_grid.fit(X, y)\n",
    "print \"after pca f1: \", pca_forest_clf_grid.best_score_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 任务5：用分类器做预测\n",
    "\n",
    "根据之上尝试过的算法来看，大致看来，使用AdaBoost的算法效果是最好，于是我们选定AdaBoost算法来作为最终算法，后面对AdaBoost算法在精细调试"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "clf:  Pipeline(steps=[('pca', PCA(copy=True, iterated_power='auto', n_components=4, random_state=None,\n",
      "  svd_solver='randomized', tol=0.0, whiten=False)), ('svc', AdaBoostClassifier(algorithm='SAMME.R', base_estimator=None,\n",
      "          learning_rate=1.0, n_estimators=20, random_state=42))])\n"
     ]
    }
   ],
   "source": [
    "\n",
    "### Task 5: Tune your classifier to achieve better than .3 precision and recall \n",
    "### using our testing script. Check the tester.py script in the final project\n",
    "### folder for details on the evaluation method, especially the test_classifier\n",
    "### function. Because of the small size of the dataset, the script uses\n",
    "### stratified shuffle split cross validation. For more info: \n",
    "### http://scikit-learn.org/stable/modules/generated/sklearn.cross_validation.StratifiedShuffleSplit.html\n",
    "\n",
    "# from sklearn.cross_validation import train_test_split\n",
    "# X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)\n",
    "\n",
    "#clf = forest_clf_grid.best_estimator_\n",
    "clf = pca_forest_clf_grid.best_estimator_\n",
    "print \"clf: \", clf"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 任务6: 把分类器，数据集，和特征列表存在本地，分别是\n",
    "\n",
    "- my_classifier.pkl 是最终选择的分类器\n",
    "- my_dataset.pkl 是数据集\n",
    "- my_feature_list.pkl 是选择的特征"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "done\n"
     ]
    }
   ],
   "source": [
    "### Task 6: Dump your classifier, dataset, and features_list so anyone can\n",
    "### check your results. You do not need to change anything below, but make sure\n",
    "### that the version of poi_id.py that you submit can be run on its own and\n",
    "### generates the necessary .pkl files for validating your results.\n",
    "\n",
    "dump_classifier_and_data(clf, my_dataset, features_list)\n",
    "print \"done\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "print \"done\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
